{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZFs9xdvZmBK",
    "outputId": "7d63b6a0-dad2-4460-a51b-0e475849d28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    " \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing relevant libraries     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT6axVXK6Zhj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable declaration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_scarped = []\n",
    "location_list = []\n",
    "author_list = []\n",
    "date_list =[]\n",
    "s =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "3X5fFQV_bWSb",
    "outputId": "b4a40a6c-ca92-499e-fca4-f9ac6d18c7a8"
   },
   "outputs": [],
   "source": [
    "# Scrapings the required information from the website\n",
    "def scraping(link):\n",
    "    raw_html = requests.get(link)\n",
    "    soup = BeautifulSoup(raw_html.text,'html.parser')\n",
    "    try:\n",
    "        location = soup.find('p',{'class':'bold'}).text\n",
    "    except AttributeError:\n",
    "        location ='9999'\n",
    "    try:\n",
    "        auther = soup.find('a',{'class': \"user\"}).text\n",
    "    except AttributeError:\n",
    "                 auther= '9999'\n",
    "    try:\n",
    "        date = soup.find('span',{'class': \"s\"}).text\n",
    "    except AttributeError:\n",
    "                 date= '9999'\n",
    "    return location, auther,date\n",
    " \n",
    "\n",
    "# Gets the required links\n",
    "def getLinks(newL,num=100):\n",
    "    global location_list \n",
    "    global author_list\n",
    "    global date_list\n",
    "    global s\n",
    "    global pages_scarped\n",
    "    pages = [] \n",
    "    html = requests.get('http://www.nairaland.com/'+newL)\n",
    "    bsObj = BeautifulSoup(html.text, 'html.parser')\n",
    " \n",
    "                    \n",
    "    for link in bsObj.findAll(\"a\", href=in re.compile(\"(news/)+[0-9]+\")):\n",
    "        if link.attrs['href'] not in pages:\n",
    "            pages.append(link.attrs['href'])\n",
    " \n",
    "    for page in pages:\n",
    "        if page not in pages_scarped: \n",
    "            html2 = requests.get('http://www.nairaland.com/'+page)\n",
    "            bsObj2 = BeautifulSoup(html2.text, 'html.parser')\n",
    "            # going  throu\n",
    "            for a in bsObj2.findAll(\"a\", href=re.compile(\"^(https://www.nairaland.com)+\\/+[0-9]+\\/.*\")):\n",
    "                location, auther,date = scraping(a.attrs['href'])\n",
    " \n",
    "                location_list.append(location)\n",
    "                author_list.append(author)\n",
    "                date_list.append(date)\n",
    "            pages_scarped.append(page)\n",
    "            print(page)\n",
    " \n",
    "    # going to the next page after the last available page is reacched\n",
    "    while s<num:\n",
    "        s += 1\n",
    "        getLinks(pages_scarped[-1])\n",
    "        \n",
    "      \n",
    "               \n",
    "getLinks('')\n",
    " \n",
    "# print(location_list)\n",
    "# print(title_list)\n",
    "#print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AT4fHRE6hud"
   },
   "outputs": [],
   "source": [
    "#exporting the scrapped data to a csv file\n",
    "df = pd.DataFrame()\n",
    "df['location']= location_list\n",
    "df['author']=author_list\n",
    "df['date']= date_list\n",
    "df.to_csv('/content/drive/MyDrive/df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j28VN3mWKeak"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WebS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
